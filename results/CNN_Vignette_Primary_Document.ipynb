{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86f0c13",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Vignette Primary Document\n",
    "Kaitlyn, Oscar, Nini, Johanna\n",
    "12/6/2025\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593b97c",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4de8d",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are a special type of neural networks designed for processing grid-structured data, particularly images. Unlike traditional fully-connected neural networks, CNNs use the spatial structure of images through localized connectivity and parameter sharing. This vignette explores CNN process and performance through implementation of a multi-class butterfly species classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60718b88",
   "metadata": {},
   "source": [
    "## Limitations of Fully Connected NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360f83e",
   "metadata": {},
   "source": [
    "Traditional neural networks face significant challenges when processing image data. For a 128×128 pixel RGB image (which is what our butterfly image dataset consists of), the input dimensionality is 49,152 (128 × 128 × 3). A single fully-connected hidden layer with 256 neurons requires 12,583,936 parameters. This parameter explosion leads to computational inefficiency and increases the risk of overfitting. Additionally, fully-connected networks lack translation equivariance. Identical patterns appearing in different spatial locations have to be learned independently, requiring substantially more training data to achieve comparable performance to CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748b1d9",
   "metadata": {},
   "source": [
    "## CNN Architecture Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0169122",
   "metadata": {},
   "source": [
    "CNNs address these limitations through two fundamental design principles:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6393e",
   "metadata": {},
   "source": [
    "### Parameter Sharing:\n",
    "\n",
    "Small learnable filters, called kernels, are applied across the entire input via convolution operations. The same filter weights are used at every spatial location, drastically reducing parameter count while enabling position-invariant feature detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c879570",
   "metadata": {},
   "source": [
    "### Hierarchical Feature Learning:\n",
    "\n",
    "Stacked convolutional layers create hierarchical representations. Initial layers extract low-level features (edges, textures), intermediate layers identify mid-level patterns (shapes, object parts), and deep layers recognize high-level semantic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6b47b",
   "metadata": {},
   "source": [
    "## CNN Layer Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dde6b",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "\n",
    "Convolutional layers apply learnable filters through discrete convolution operations. Each filter performs element-wise multiplication with the receptive field and sums the results, producing activation maps that indicate feature presence at different spatial locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2068086",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Non-linear activation functions, typically ReLU (Rectified Linear Unit: f(x) = max(0,x)), are applied element-wise following convolution operations. Non-linearity is essential for learning complex mappings beyond linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d933ef",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "Pooling operations perform spatial downsampling. Max pooling selects the maximum value within each pooling window, reducing spatial dimensions while retaining salient features. This provides computational efficiency, partial translation invariance, and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4494bd",
   "metadata": {},
   "source": [
    "### Flatten Layers\n",
    "\n",
    "Flatten operations reshape multi-dimensional feature maps into one-dimensional vectors, enabling transition from convolutional feature extraction to fully-connected classification layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01e8f4",
   "metadata": {},
   "source": [
    "### Fully-Connected Layers\n",
    "\n",
    "Dense layers with full connectivity between consecutive layers aggregate extracted features for final classification. These layers function identically to traditional neural network layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40f83a",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout applies stochastic regularization by randomly deactivating neurons during training with specified probability. This prevents co-adaptation of features and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604aaf2",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "The final layer uses softmax activation to produce normalized probability distributions over classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0861e84",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The dataset comprises butterfly images spanning 75 species classes. The training set contains 6,499 images, split into 5,199 training samples and 1,300 validation samples (80/20 ratio). The test set contains 2,786 images. All images are resized to 128×128 pixels and processed in batches of 32. Stratified sampling ensures proportional class representation across training and validation splits. This is critical for maintaining class balance in multi-class problems with potentially uneven class distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7ad9696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display:flex;\">\n",
       "  <img src=\"../data/test/Image_1.jpg\" width=\"200\">\n",
       "  <img src=\"../data/test/Image_2.jpg\" width=\"200\">\n",
       "  <img src=\"../data/test/Image_3.jpg\" width=\"200\">\n",
       "  <img src=\"../data/test/Image_4.jpg\" width=\"200\">\n",
       "  <img src=\"../data/test/Image_5.jpg\" width=\"200\">\n",
       "  <img src=\"../data/test/Image_6.jpg\" width=\"200\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display:flex;\">\n",
    "  <img src=\"../data/test/Image_1.jpg\" width=\"200\">\n",
    "  <img src=\"../data/test/Image_2.jpg\" width=\"200\">\n",
    "  <img src=\"../data/test/Image_3.jpg\" width=\"200\">\n",
    "  <img src=\"../data/test/Image_4.jpg\" width=\"200\">\n",
    "  <img src=\"../data/test/Image_5.jpg\" width=\"200\">\n",
    "  <img src=\"../data/test/Image_6.jpg\" width=\"200\">\n",
    "</div>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717ab1c",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The implemented CNN follows a sequential architecture with variable depth controlled by the build_cnn() function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7993881",
   "metadata": {},
   "source": [
    "### Input Layer:\n",
    "\n",
    "128×128×3 (RGB images)\n",
    "\n",
    "### Convolutional Blocks:\n",
    "\n",
    "Each block contains a Conv2D layer with ReLU activation followed by a 2x2 max pooling. The number of filters doubles with each block (starting from 32).\n",
    "\n",
    "### Flatten Layer:\n",
    "\n",
    "Converts 3D feature maps to 1D vector\n",
    "\n",
    "### Dense Layer:\n",
    "\n",
    "256 units, ReLU activation \n",
    "\n",
    "### Dropout Layer: \n",
    "\n",
    "50% dropout rate\n",
    "\n",
    "### Output Layer:\n",
    "\n",
    "75 units (one per class), softmax activation\n",
    "\n",
    "The model contains 6,535,307 trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3bbba",
   "metadata": {},
   "source": [
    "## Depth Analysis\n",
    "\n",
    "An experiment examining network depth trained models with 1, 2, 3, and 4 convolutional blocks:\n",
    "\n",
    "### 1 Block:\n",
    "\n",
    "Validation accuracy of 29.3%. Insufficient representational capacity for complex multi-class classification.The model struggles to learn meaningful hierarchical features with only one layer of abstraction.\n",
    "\n",
    "### 2 Blocks:\n",
    "\n",
    "Validation accuracy of 49.8%. Additional depth enables more discriminative feature learning. The second convolutional block allows the network to combine low-level features into more complex patterns.\n",
    "\n",
    "### 3 Blocks:\n",
    "\n",
    "Validation accuracy of 60.6%. Three-layer hierarchy provides adequate feature abstraction for this task, enabling the network to capture more sophisticated visual patterns that distinguish butterfly species.\n",
    "\n",
    "### 4 Blocks:\n",
    "\n",
    "Validation accuracy of 62.2%. Four convolutional blocks achieve the highest performance, demonstrating that additional depth contineus to benefit the model on this dataset. The deeper architecture can learn more nuanced hierarchical representation.\n",
    "\n",
    "These results demonstrate that network depth directly impacts feature abstraction capability. However, excessive depth may lead to overfitting or training difficulties without sufficient data or regularization. Each additional convolutional block improves validation accuracy, showing that deeper networks can learn more sophisticated feature hierarchies for distinguishing between 75 butterfly species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da13166",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "All model were trained for 10 epochs using the Adam optimizer and categorical cross-entropy loss. Training progression showed:\n",
    "\n",
    "Epoch 1: 7.5% validation accuracy\n",
    "\n",
    "Epoch 10: 62.2% validation accuracy\n",
    "\n",
    "Training accuracy reached approximately 73.5% for the 4-block model by epoch 10, indicating some degree of overfitting. The training-validation accuracy gap suggests potential benefit from additional regularization or larger training datasets. This could be addressed through additional regularization techniques, data augmentation, or larger training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4a091",
   "metadata": {},
   "source": [
    "## Validation Predictions and Visualization\n",
    "\n",
    "After training, the final model (4-block CNN) was evaluated on the validation set, achieving 62.2% accuracy. Sample predictions were visualized showing both correct and incorrect classifications, providing insight into model performance and common misclassification patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075aa21c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "CNNs provide substantial advantages over fully-connected architectures for image classification tasks through parameter sharing and spatial structure exploitation. The butterfly classifier achieved 62.2% accuracy across 75 classes with the 4-block architecture, demonstrating effective feature learning from training data. Network depth proved critical for performance, with deeper architectures enabling more sophisticated feature hierarchies. Further improvements could be obtained through transfer learning, additional regularization techniques, or larger training datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a72a5",
   "metadata": {},
   "source": [
    "# Code Summary\n",
    "\n",
    "## Library Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5798e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb80e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/Training_set.csv\")\n",
    "test_df  = pd.read_csv(\"../data/Testing_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd98863",
   "metadata": {},
   "source": [
    "Here, we imported the necessary libraries for data manipulation (pandas, numpy), deep learning (tensorflow/keras), and image processing. We then loaded the training and testing CSV files that contained the image filenames and their species labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abff52",
   "metadata": {},
   "source": [
    "## Image Configuration and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "train_df = train_df.rename(columns={\"Image\": \"filename\", \"label\": \"class\"})\n",
    "\n",
    "train_df_split, val_df_split = train_test_split(\n",
    "    train_df, test_size=0.2, stratify=train_df['class'], random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce7a3c",
   "metadata": {},
   "source": [
    "We set the image dimensions to 128x128 pixels and batch size to 32 images per training iteration, and renamed the dataframe columns. We split the training data set into 80% training and 20% validation subsets. The stratify parameter makes sure that each butterfly parameter maintains proportional representation in both splits, which is essential for balanced multi-class learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aefdb6",
   "metadata": {},
   "source": [
    "## Data Normalization and Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57dd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(...)\n",
    "val_gen = val_datagen.flow_from_dataframe(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae9085",
   "metadata": {},
   "source": [
    "Creates ImageDataGenerator objects that normalize pixel values from [0, 255] to [0, 1]. Unlike the previous version, this code does not apply data augmentation, keeping the preprocessing simple with only normalization. Generators automatically load, preprocess, and batch images from directories during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57badefa",
   "metadata": {},
   "source": [
    "## Variable Depth CNN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(num_conv_blocks=3, base_filters=32, dense_units=256):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(base_filters, (3,3), activation='relu', \n",
    "                            input_shape=(*IMAGE_SIZE, 3)))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    filters = base_filters\n",
    "    for i in range(num_conv_blocks - 1):\n",
    "        filters *= 2\n",
    "        model.add(layers.Conv2D(filters, (3,3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80acd3fd",
   "metadata": {},
   "source": [
    "Defines a function to construct CNNs with configurable depth. Allows experimentation with different numbers of convolutional blocks while maintaining consistent architecture patterns (doubling filters each block, same pooling strategy). The function builds the first convolutional block with the specified base filters, then iteratively adds additional blocks with progressively more filters (32 → 64 → 128 → 256 for a 4-block model). Enables systematic comparison of model complexity versus performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e899f",
   "metadata": {},
   "source": [
    "## Depth Experiment Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "depths = [1, 2, 3, 4]\n",
    "histories = {}\n",
    "\n",
    "for d in depths:\n",
    "    print(f\"\\nTraining model with {d} conv blocks...\\n\")\n",
    "    model = build_cnn(num_conv_blocks=d)\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS)\n",
    "    histories[d] = history\n",
    "    \n",
    "    val_loss, val_acc = model.evaluate(val_gen)\n",
    "    print(f\"Validation accuracy for depth {d}: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2f904",
   "metadata": {},
   "source": [
    "Trains multiple models with 1, 2, 3, and 4 convolutional blocks to analyze the relationship between network depth and classification performance. Stores training history for each configuration and evaluates final validation accuracy. Results show validation accuracy increasing from 29.3% (1 block) to 49.8% (2 blocks) to 60.6% (3 blocks) to 62.2% (4 blocks), demonstrating the importance of depth for feature abstraction in this multi-class classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcbe223",
   "metadata": {},
   "source": [
    "## Validation Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probs = model.predict(val_gen)\n",
    "val_pred_indices = np.argmax(val_probs, axis=1)\n",
    "index_to_class = {v: k for k, v in train_gen.class_indices.items()}\n",
    "val_pred_labels = [index_to_class[i] for i in val_pred_indices]\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_gen)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03d8741d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display:flex;\">\n",
       "  <img src=\"../data/train/ButterflyPredictions1.png\" width=\"200\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"\"\"\n",
    "<div style=\"display:flex;\">\n",
    "  <img src=\"../data/train/ButterflyPredictions1.png\" width=\"200\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345fe7f",
   "metadata": {},
   "source": [
    "Generates predictions on the validation set by computing class probability distributions for all 1,300 validation images at once. Selects the highest probability class for each image using argmax, then maps numeric indices back to butterfly species names. Evaluates final model performance, achieving 62.2% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca9204",
   "metadata": {},
   "source": [
    "## Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "N = 12\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i in range(N):\n",
    "    row = val_df_split.iloc[i]\n",
    "    img_path = os.path.join(TRAIN_DIR, row[\"filename\"])\n",
    "    img = plt.imread(img_path)\n",
    "    \n",
    "    true_label = row[\"class\"]\n",
    "    pred_label = val_pred_labels[i]\n",
    "    \n",
    "    correct = (pred_label == true_label)\n",
    "    mark = \"✓\" if correct else \"✗\"\n",
    "    title = f\"True: {true_label}\\nPred: {pred_label} {mark}\"\n",
    "    \n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
