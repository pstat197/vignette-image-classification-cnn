{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      filename                     label\n",
      "0  Image_1.jpg          SOUTHERN DOGFACE\n",
      "1  Image_2.jpg                    ADONIS\n",
      "2  Image_3.jpg            BROWN SIPROETA\n",
      "3  Image_4.jpg                   MONARCH\n",
      "4  Image_5.jpg  GREEN CELLED CATTLEHEART\n",
      "      filename\n",
      "0  Image_1.jpg\n",
      "1  Image_2.jpg\n",
      "2  Image_3.jpg\n",
      "3  Image_4.jpg\n",
      "4  Image_5.jpg\n",
      "75 classes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "TRAIN_CSV = \"../data/Training_set.csv\"\n",
    "TEST_CSV  = \"../data/Testing_set.csv\"\n",
    "TRAIN_DIR = \"../data/train\"\n",
    "TEST_DIR  = \"../data/test\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df['label'].nunique(), \"classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5199 validated image filenames belonging to 75 classes.\n",
      "Found 1300 validated image filenames belonging to 75 classes.\n",
      "Number of classes: 75\n",
      "Found 2786 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_df = train_df.rename(columns={\"Image\": \"filename\", \"label\": \"class\"})\n",
    "test_df  = test_df.rename(columns={\"Image\": \"filename\"})\n",
    "\n",
    "# Train/validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df_split, val_df_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    stratify=train_df['class'],\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    train_df_split,\n",
    "    directory=TRAIN_DIR,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"class\",\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_dataframe(\n",
    "    val_df_split,\n",
    "    directory=TRAIN_DIR,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"class\",\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_df_split[\"class\"].nunique()\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Test generator – IMPORTANT: shuffle=False to preserve CSV order\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    directory=TEST_DIR,\n",
    "    x_col=\"filename\",\n",
    "    y_col=None,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_cnn(num_conv_blocks=3, base_filters=32, dense_units=256):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First block (requires input shape)\n",
    "    model.add(layers.Conv2D(base_filters, (3,3), activation='relu', \n",
    "                            input_shape=(*IMAGE_SIZE, 3)))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    # Additional blocks\n",
    "    filters = base_filters\n",
    "    for i in range(num_conv_blocks - 1):\n",
    "        filters *= 2\n",
    "        model.add(layers.Conv2D(filters, (3,3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with 1 conv blocks...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 161ms/step - accuracy: 0.0130 - loss: 6.1498 - val_accuracy: 0.0200 - val_loss: 4.3135\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 206ms/step - accuracy: 0.0165 - loss: 4.3166 - val_accuracy: 0.0185 - val_loss: 4.2953\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 208ms/step - accuracy: 0.0195 - loss: 4.2890 - val_accuracy: 0.0354 - val_loss: 4.1174\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 230ms/step - accuracy: 0.0358 - loss: 4.1491 - val_accuracy: 0.0638 - val_loss: 4.0484\n",
      "Epoch 5/10\n",
      "\u001b[1m109/163\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 203ms/step - accuracy: 0.0345 - loss: 4.0605"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "depths = [1, 2, 3, 4]   # try what you want\n",
    "histories = {}\n",
    "\n",
    "for d in depths:\n",
    "    print(f\"\\nTraining model with {d} conv blocks...\\n\")\n",
    "    model = build_cnn(num_conv_blocks=d)\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "    histories[d] = history\n",
    "\n",
    "    val_loss, val_acc = model.evaluate(val_gen)\n",
    "    print(f\"Validation accuracy for depth {d}: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step\n",
      "      filename         predicted_label\n",
      "0  Image_1.jpg              PINE WHITE\n",
      "1  Image_2.jpg           CRIMSON PATCH\n",
      "2  Image_3.jpg      RED SPOTTED PURPLE\n",
      "3  Image_4.jpg         IPHICLUS SISTER\n",
      "4  Image_5.jpg  MILBERTS TORTOISESHELL\n"
     ]
    }
   ],
   "source": [
    "# Predict class probabilities\n",
    "pred_probs = model.predict(test_gen)\n",
    "\n",
    "# Convert to class indices\n",
    "pred_indices = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Map indices back to labels (strings)\n",
    "index_to_class = {v: k for k, v in train_gen.class_indices.items()}\n",
    "pred_labels = [index_to_class[i] for i in pred_indices]\n",
    "\n",
    "# Attach to test_df in the same order\n",
    "test_df['predicted_label'] = pred_labels\n",
    "\n",
    "# Save for the competition / your vignette\n",
    "test_df.to_csv(\"../results/butterfly_predictions.csv\", index=False)\n",
    "print(test_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
