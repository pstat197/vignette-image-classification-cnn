{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      filename                     label\n",
      "0  Image_1.jpg          SOUTHERN DOGFACE\n",
      "1  Image_2.jpg                    ADONIS\n",
      "2  Image_3.jpg            BROWN SIPROETA\n",
      "3  Image_4.jpg                   MONARCH\n",
      "4  Image_5.jpg  GREEN CELLED CATTLEHEART\n",
      "      filename\n",
      "0  Image_1.jpg\n",
      "1  Image_2.jpg\n",
      "2  Image_3.jpg\n",
      "3  Image_4.jpg\n",
      "4  Image_5.jpg\n",
      "75 classes\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the CSVs containing image filenames and labels\n",
    "TRAIN_CSV = \"../data/Training_set.csv\"\n",
    "TEST_CSV  = \"../data/Testing_set.csv\"\n",
    "TRAIN_DIR = \"../data/train\"\n",
    "TEST_DIR  = \"../data/test\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df['label'].nunique(), \"classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5199 validated image filenames belonging to 75 classes.\n",
      "Found 1300 validated image filenames belonging to 75 classes.\n",
      "Number of classes: 75\n",
      "Found 2786 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn into Keras format: with \"filename\" and \"class\" as column names\n",
    "train_df = train_df.rename(columns={\"Image\": \"filename\", \"label\": \"class\"})\n",
    "test_df  = test_df.rename(columns={\"Image\": \"filename\"})\n",
    "\n",
    "# Split train data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df_split, val_df_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    stratify=train_df['class'],\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Image normalizing (rescale pixel values 0–255 -> 0–1)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators that read images directly from folders\n",
    "# Keras loads + preprocesses images for us automatically\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    train_df_split,\n",
    "    directory=TRAIN_DIR,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"class\",\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_dataframe(\n",
    "    val_df_split,\n",
    "    directory=TRAIN_DIR,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"class\",\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Total number of output classes\n",
    "num_classes = train_df_split[\"class\"].nunique()\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# Test generator – IMPORTANT: shuffle=False to preserve CSV order\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    directory=TEST_DIR,\n",
    "    x_col=\"filename\",\n",
    "    y_col=None,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Function to build a CNN with variable depth\n",
    "# Allows us to test how model complexity affects performance\n",
    "def build_cnn(num_conv_blocks=3, base_filters=32, dense_units=256):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First block (requires input shape)\n",
    "    model.add(layers.Conv2D(base_filters, (3,3), activation='relu', \n",
    "                            input_shape=(*IMAGE_SIZE, 3)))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "    # Additional convolution blocks (filters double each block)\n",
    "    filters = base_filters\n",
    "    for i in range(num_conv_blocks - 1):\n",
    "        filters *= 2\n",
    "        model.add(layers.Conv2D(filters, (3,3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2,2)))\n",
    "    \n",
    "    # Classification head\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model# Train multiple CNNs with different depths\n",
    "# (1, 2, 3, 4 convolution blocks)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with 1 conv blocks...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 152ms/step - accuracy: 0.0184 - loss: 8.2808 - val_accuracy: 0.0162 - val_loss: 4.2779\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 255ms/step - accuracy: 0.0218 - loss: 4.2558 - val_accuracy: 0.0277 - val_loss: 4.2479\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 303ms/step - accuracy: 0.0213 - loss: 4.1861 - val_accuracy: 0.0569 - val_loss: 3.9935\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 180ms/step - accuracy: 0.0377 - loss: 4.0367 - val_accuracy: 0.0885 - val_loss: 3.7986\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 312ms/step - accuracy: 0.0533 - loss: 3.9017 - val_accuracy: 0.1154 - val_loss: 3.5868\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 213ms/step - accuracy: 0.0756 - loss: 3.7349 - val_accuracy: 0.1354 - val_loss: 3.4108\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 275ms/step - accuracy: 0.0875 - loss: 3.5930 - val_accuracy: 0.1777 - val_loss: 3.1965\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 298ms/step - accuracy: 0.1279 - loss: 3.2962 - val_accuracy: 0.2323 - val_loss: 3.0440\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 241ms/step - accuracy: 0.1781 - loss: 3.0483 - val_accuracy: 0.2600 - val_loss: 2.8271\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 213ms/step - accuracy: 0.2181 - loss: 2.8281 - val_accuracy: 0.2931 - val_loss: 2.7195\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.2892 - loss: 2.7206\n",
      "Validation accuracy for depth 1: 0.2931\n",
      "\n",
      "Training model with 2 conv blocks...\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 236ms/step - accuracy: 0.0388 - loss: 4.3287 - val_accuracy: 0.2054 - val_loss: 3.4021\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 295ms/step - accuracy: 0.2328 - loss: 3.1043 - val_accuracy: 0.4108 - val_loss: 2.4286\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 305ms/step - accuracy: 0.4828 - loss: 1.9503 - val_accuracy: 0.4600 - val_loss: 2.0663\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 233ms/step - accuracy: 0.6558 - loss: 1.2736 - val_accuracy: 0.4892 - val_loss: 1.9832\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 236ms/step - accuracy: 0.7896 - loss: 0.7476 - val_accuracy: 0.4800 - val_loss: 2.0993\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 279ms/step - accuracy: 0.8509 - loss: 0.5466 - val_accuracy: 0.4738 - val_loss: 2.2045\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 251ms/step - accuracy: 0.8904 - loss: 0.3854 - val_accuracy: 0.5023 - val_loss: 2.1388\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 266ms/step - accuracy: 0.9183 - loss: 0.2843 - val_accuracy: 0.4962 - val_loss: 2.3054\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 278ms/step - accuracy: 0.9251 - loss: 0.2862 - val_accuracy: 0.4900 - val_loss: 2.3285\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 280ms/step - accuracy: 0.9422 - loss: 0.2059 - val_accuracy: 0.4977 - val_loss: 2.4296\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.4910 - loss: 2.5088\n",
      "Validation accuracy for depth 2: 0.4977\n",
      "\n",
      "Training model with 3 conv blocks...\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 306ms/step - accuracy: 0.0371 - loss: 4.1864 - val_accuracy: 0.2746 - val_loss: 3.0186\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 282ms/step - accuracy: 0.2489 - loss: 2.9898 - val_accuracy: 0.4292 - val_loss: 2.2133\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 281ms/step - accuracy: 0.4215 - loss: 2.1694 - val_accuracy: 0.4892 - val_loss: 1.8800\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 642ms/step - accuracy: 0.5264 - loss: 1.6998 - val_accuracy: 0.5469 - val_loss: 1.7164\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 465ms/step - accuracy: 0.6524 - loss: 1.2079 - val_accuracy: 0.5738 - val_loss: 1.5788\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 417ms/step - accuracy: 0.7189 - loss: 0.9496 - val_accuracy: 0.5738 - val_loss: 1.6167\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 433ms/step - accuracy: 0.7968 - loss: 0.6952 - val_accuracy: 0.5869 - val_loss: 1.6432\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 428ms/step - accuracy: 0.8317 - loss: 0.5524 - val_accuracy: 0.5769 - val_loss: 1.7041\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 518ms/step - accuracy: 0.8621 - loss: 0.4424 - val_accuracy: 0.6138 - val_loss: 1.7192\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 527ms/step - accuracy: 0.8730 - loss: 0.4003 - val_accuracy: 0.6062 - val_loss: 1.7987\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 139ms/step - accuracy: 0.6042 - loss: 1.9023\n",
      "Validation accuracy for depth 3: 0.6062\n",
      "\n",
      "Training model with 4 conv blocks...\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 677ms/step - accuracy: 0.0196 - loss: 4.2899 - val_accuracy: 0.0746 - val_loss: 3.8488\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 508ms/step - accuracy: 0.1003 - loss: 3.6971 - val_accuracy: 0.2969 - val_loss: 2.7282\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 539ms/step - accuracy: 0.2443 - loss: 2.9483 - val_accuracy: 0.4123 - val_loss: 2.2431\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 536ms/step - accuracy: 0.3621 - loss: 2.3563 - val_accuracy: 0.4908 - val_loss: 1.8579\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 527ms/step - accuracy: 0.4682 - loss: 1.8967 - val_accuracy: 0.5346 - val_loss: 1.7160\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 601ms/step - accuracy: 0.5317 - loss: 1.6174 - val_accuracy: 0.5669 - val_loss: 1.5920\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 570ms/step - accuracy: 0.5988 - loss: 1.3815 - val_accuracy: 0.6062 - val_loss: 1.4867\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 459ms/step - accuracy: 0.6482 - loss: 1.1814 - val_accuracy: 0.6015 - val_loss: 1.5061\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 383ms/step - accuracy: 0.7125 - loss: 0.9595 - val_accuracy: 0.6185 - val_loss: 1.4097\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 334ms/step - accuracy: 0.7345 - loss: 0.8635 - val_accuracy: 0.6215 - val_loss: 1.4096\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.6125 - loss: 1.4776\n",
      "Validation accuracy for depth 4: 0.6215\n"
     ]
    }
   ],
   "source": [
    "# Train multiple CNNs with different depths\n",
    "# (1, 2, 3, 4 convolution blocks)\n",
    "EPOCHS = 10\n",
    "depths = [1, 2, 3, 4]   # try what you want\n",
    "histories = {}\n",
    "\n",
    "for d in depths:\n",
    "    print(f\"\\nTraining model with {d} conv blocks...\\n\")\n",
    "    model = build_cnn(num_conv_blocks=d)\n",
    "    # Train and store history for comparing performance later\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "    histories[d] = history\n",
    "\n",
    "    val_loss, val_acc = model.evaluate(val_gen)\n",
    "    print(f\"Validation accuracy for depth {d}: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 94ms/step\n",
      "      filename         predicted_label\n",
      "0  Image_1.jpg              PINE WHITE\n",
      "1  Image_2.jpg           CRIMSON PATCH\n",
      "2  Image_3.jpg                  ADONIS\n",
      "3  Image_4.jpg         IPHICLUS SISTER\n",
      "4  Image_5.jpg  MILBERTS TORTOISESHELL\n"
     ]
    }
   ],
   "source": [
    "# Predict class probabilities\n",
    "# Make predictions using the LAST trained model\n",
    "pred_probs = model.predict(test_gen)\n",
    "\n",
    "# Convert probabilities -> predicted class indices\n",
    "pred_indices = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Map indices back to labels (strings)\n",
    "index_to_class = {v: k for k, v in train_gen.class_indices.items()}\n",
    "pred_labels = [index_to_class[i] for i in pred_indices]\n",
    "\n",
    "# Attach to test_df in the same order\n",
    "test_df['predicted_label'] = pred_labels\n",
    "\n",
    "# Save to csv file\n",
    "test_df.to_csv(\"../results/butterfly_predictions.csv\", index=False)\n",
    "print(test_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
