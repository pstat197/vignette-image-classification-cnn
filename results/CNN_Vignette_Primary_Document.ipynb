{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86f0c13",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Vignette Primary Document\n",
    "Kaitlyn, Oscar, Nini, Johanna\n",
    "12/6/2025\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593b97c",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4de8d",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are a special type of neural networks designed for processing grid-structured data, particularly images. Unlike traditional fully-connected neural networks, CNNs use the spatial structure of images through localized connectivity and parameter sharing. This vignette explores CNN process and performance through implementation of a multi-class butterfly species classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60718b88",
   "metadata": {},
   "source": [
    "## Limitations of Fully Connected NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360f83e",
   "metadata": {},
   "source": [
    "Traditional neural networks face significant challenges when processing image data. For a 128×128 pixel RGB image (which is what our butterfly image dataset consists of), the input dimensionality is 49,152 (128 × 128 × 3). A single fully-connected hidden layer with 256 neurons requires 12,583,936 parameters. This parameter explosion leads to computational inefficiency and increases the risk of overfitting. Additionally, fully-connected networks lack translation equivariance. Identical patterns appearing in different spatial locations have to be learned independently, requiring substantially more training data to achieve comparable performance to CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748b1d9",
   "metadata": {},
   "source": [
    "## CNN Architecture Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0169122",
   "metadata": {},
   "source": [
    "CNNs address these limitations through two fundamental design principles:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6393e",
   "metadata": {},
   "source": [
    "### Parameter Sharing:\n",
    "\n",
    "Small learnable filters, called kernels, are applied across the entire input via convolution operations. The same filter weights are used at every spatial location, drastically reducing parameter count while enabling position-invariant feature detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c879570",
   "metadata": {},
   "source": [
    "### Hierarchical Feature Learning:\n",
    "\n",
    "Stacked convolutional layers create hierarchical representations. Initial layers extract low-level features (edges, textures), intermediate layers identify mid-level patterns (shapes, object parts), and deep layers recognize high-level semantic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6b47b",
   "metadata": {},
   "source": [
    "## CNN Layer Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dde6b",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "\n",
    "Convolutional layers apply learnable filters through discrete convolution operations. Each filter performs element-wise multiplication with the receptive field and sums the results, producing activation maps that indicate feature presence at different spatial locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2068086",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Non-linear activation functions, typically ReLU (Rectified Linear Unit: f(x) = max(0,x)), are applied element-wise following convolution operations. Non-linearity is essential for learning complex mappings beyond linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d933ef",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "Pooling operations perform spatial downsampling. Max pooling selects the maximum value within each pooling window, reducing spatial dimensions while retaining salient features. This provides computational efficiency, partial translation invariance, and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4494bd",
   "metadata": {},
   "source": [
    "### Flatten Layers\n",
    "\n",
    "Flatten operations reshape multi-dimensional feature maps into one-dimensional vectors, enabling transition from convolutional feature extraction to fully-connected classification layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01e8f4",
   "metadata": {},
   "source": [
    "### Fully-Connected Layers\n",
    "\n",
    "Dense layers with full connectivity between consecutive layers aggregate extracted features for final classification. These layers function identically to traditional neural network layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40f83a",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout applies stochastic regularization by randomly deactivating neurons during training with specified probability. This prevents co-adaptation of features and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604aaf2",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "The final layer uses softmax activation to produce normalized probability distributions over classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0861e84",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The dataset comprises butterfly images spanning 75 species classes. The training set contains 6,499 images, split into 5,199 training samples and 1,300 validation samples (80/20 ratio). The test set contains 2,786 images. All images are resized to 128×128 pixels and processed in batches of 32. Stratified sampling ensures proportional class representation across training and validation splits. This is critical for maintaining class balance in multi-class problems with potentially uneven class distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e22d092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <img src=\"../data/test/Image_1.jpg\" width=\"200\">\n",
       "  <img src=\"../data/test/Image_2.jpg\" width=\"200\">\n",
       "  <img src=\"../data/test/Image_3.jpg\" width=\"200\">    \n",
       "  <img src=\"../data/test/Image_4.jpg\" width=\"200\">\n",
       "  <img src=\"../data/test/Image_5.jpg\" width=\"200\"> \n",
       "  <img src=\"../data/test/Image_6.jpg\" width=\"200\">    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "  <img src=\"../data/test/Image_1.jpg\" width=\"200\">\n",
    "  <img src=\"../data/test/Image_2.jpg\" width=\"200\">\n",
    "  <img src=\"../data/test/Image_3.jpg\" width=\"200\">    \n",
    "  <img src=\"../data/test/Image_4.jpg\" width=\"200\">\n",
    "  <img src=\"../data/test/Image_5.jpg\" width=\"200\"> \n",
    "  <img src=\"../data/test/Image_6.jpg\" width=\"200\">    \n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717ab1c",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The implemented CNN follows a sequential architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7993881",
   "metadata": {},
   "source": [
    "### Input Layer:\n",
    "\n",
    "128×128×3 (RGB images)\n",
    "\n",
    "### Convolutional Block 1:\n",
    "\n",
    "32 filters (3×3), ReLU activation, followed by 2×2 max pooling\n",
    "\n",
    "### Convolutional Block 2:\n",
    "\n",
    "64 filters (3×3), ReLU activation, followed by 2×2 max pooling\n",
    "\n",
    "### Convolutional Block 3:\n",
    "\n",
    "128 filters (3×3), ReLU activation, followed by 2×2 max pooling\n",
    "\n",
    "### Flatten Layer:\n",
    "\n",
    "Converts 3D feature maps to 1D vector\n",
    "\n",
    "### Dense Layer:\n",
    "\n",
    "256 units, ReLU activation Dropout Layer: 50% dropout rate\n",
    "\n",
    "### Output Layer:\n",
    "\n",
    "75 units (one per class), softmax activation\n",
    "\n",
    "The model contains 6,535,307 trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3bbba",
   "metadata": {},
   "source": [
    "## Depth Analysis\n",
    "\n",
    "An experiment examining network depth trained models with 1, 2, 3, and 4 convolutional blocks:\n",
    "\n",
    "### 1 Block:\n",
    "\n",
    "Validation accuracy of 31.6%. Insufficient representational capacity for complex multi-class classification.\n",
    "\n",
    "### 2 Blocks:\n",
    "\n",
    "Validation accuracy of 55.7%. Additional depth enables more discriminative feature learning.\n",
    "\n",
    "### 3 Blocks:\n",
    "\n",
    "Validation accuracy of 67.2%. Three-layer hierarchy provides adequate feature abstraction for this task.\n",
    "\n",
    "These results demonstrate that network depth directly impacts feature abstraction capability. However, excessive depth may lead to overfitting or training difficulties without sufficient data or regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da13166",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "The model was trained for 10 epochs using the Adam optimizer and categorical cross-entropy loss. Training progression showed:\n",
    "\n",
    "Epoch 1: 17.8% validation accuracy\n",
    "\n",
    "Epoch 10: 67.2% validation accuracy\n",
    "\n",
    "Training accuracy reached approximately 88-89% for the 2-block model, indicating some degree of overfitting. The training-validation accuracy gap suggests potential benefit from additional regularization or larger training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4a091",
   "metadata": {},
   "source": [
    "## Prediction Pipeline\n",
    "\n",
    "Test set predictions follow this procedure:\n",
    "\n",
    "1. Forward pass through trained model\n",
    "\n",
    "2. Softmax output produces probability distribution over 75 classes\n",
    "\n",
    "3. Class with maximum probability selected via argmax operation\n",
    "\n",
    "4. Numeric class indices mapped to species labels\n",
    "\n",
    "5. Results exported to CSV with filename and predicted species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075aa21c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "CNNs provide substantial advantages over fully-connected architectures for image classification tasks through parameter sharing and spatial structure exploitation. The butterfly classifier achieved 67.2% accuracy across 75 classes, demonstrating effective feature learning from training data. Network depth proved critical for performance, with deeper architectures enabling more sophisticated feature hierarchies. Further improvements could be obtained through transfer learning, additional regularization techniques, or larger training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a72a5",
   "metadata": {},
   "source": [
    "# Code Summary\n",
    "\n",
    "## Library Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5798e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb80e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/Training_set.csv\")\n",
    "test_df  = pd.read_csv(\"../data/Testing_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd98863",
   "metadata": {},
   "source": [
    "Here, we imported the necessary libraries for data manipulation (pandas, numpy), deep learning (tensorflow/keras), and image processing. We then loaded the training and testing CSV files that contained the image filenames and their species labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abff52",
   "metadata": {},
   "source": [
    "## Image Configuration and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "train_df = train_df.rename(columns={\"Image\": \"filename\", \"label\": \"class\"})\n",
    "\n",
    "train_df_split, val_df_split = train_test_split(\n",
    "    train_df, test_size=0.2, stratify=train_df['class'], random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce7a3c",
   "metadata": {},
   "source": [
    "We set the image dimensions to 128x128 pixels and batch size to 32 images per training iteration, and renamed the dataframe columns. We split the training data set into 80% training and 20% validation subsets. The stratify parameter makes sure that each butterfly parameter maintains proportional representation in both splits, which is essential for balanced multi-class learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2db52",
   "metadata": {},
   "source": [
    "## Model Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19895b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)), ...]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0d4fe",
   "metadata": {},
   "source": [
    "This chunk constructs a sequential CNN with three convolutional blocks. Each block contains a Conv2D layer (with 32, 64, then 128 filters respectively) followed by MaxPooling2D. After feature extraction, a Flatten layer converts 3D feature maps to 1D, followed by a Dense layer (256 units), Dropout (0.5), and final Dense output layer (75 units with softmax) for class probability prediction. Total: 6,535,307 trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8eb54",
   "metadata": {},
   "source": [
    "## Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=\"adam\",\n",
    "  loss=\"categorical_crossentropy\",\n",
    "  metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab8d71",
   "metadata": {},
   "source": [
    "Here, we configure the model for training using the Adam optimizer (adaptive learning rate), categorical cross-entropy loss function (appropriate for multi-class classification with one-hot encoded labels), and accuracy as the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba7468",
   "metadata": {},
   "source": [
    "## Variable Depth CNN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(num_conv_blocks=3,\n",
    "    base_filters=32, dense_units=256): \n",
    "      model = models.Sequential() \n",
    "      ...\n",
    "   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b30182",
   "metadata": {},
   "source": [
    "This chunk defines a function to construct CNNs with configurable depth. It allows experimentation with different numbers of convolutional blocks while maintaining consistent architecture patterns (i.e. doubling filters each block, same pooling strategy). It also enables systematic comparison of model complexity versus performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e899f",
   "metadata": {},
   "source": [
    "## Depth Experiment Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in depths:\n",
    "    print(f\"\\nTraining model with {d} conv blocks...\\n\") model #=\n",
    "    build_cnn(num_conv_blocks=d) history = model.fit(train_gen,\n",
    "    validation_data=val_gen, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2f904",
   "metadata": {},
   "source": [
    "Here, we trained multiple models with 1, 2, and 3 convolutional blocks to analyze the relationship between network depth and classification performance. This chunk stores training history for each configuration, and results show validation accuracy increasing from 31.6% (1 block) to 55.7% (2 blocks), demonstrating the importance of depth for feature abstraction. Training was interrupted before completing the 3-block model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcbe223",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10 history = model.fit( train_gen,\n",
    "    validation_data=val_gen, epochs=EPOCHS ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345fe7f",
   "metadata": {},
   "source": [
    "This chunk trains the model for 10 epochs using the training generator and validates performance on the validation set after each epoch. Training history records loss and accuracy metrics, showing improvement from 17.8% validation accuracy (epoch 1) to 67.2% (epoch 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca9204",
   "metadata": {},
   "source": [
    "## Prediction and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = model.predict(test_gen)\n",
    "    pred_indices = np.argmax(pred_probs, axis=1) pred_labels =\n",
    "    [index_to_class[i] for i in pred_indices] test_df['predicted_label']\n",
    "    = pred_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf99a5a",
   "metadata": {},
   "source": [
    "Here, we generated predictions on the 2,786 test images by computing class probability distributions, selecting the highest probability class for each image, and mapping numeric indices back to butterfly species names. Predictions are appended to the test dataframe and exported to CSV file maintaining original image order (shuffle=False ensures correct alignment)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
