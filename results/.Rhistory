val.error = mean(pred.val != dat.val$Direction))
}
folds = cut(1:nrow(dat), breaks = 10)
folds = sample(folds)
cv.results = sapply(1:10, do.chunk, folddef = folds, dat = dat)
cv.error = mean(cv.results[[2]])
cv.error
# try another axis scale
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy)) +
geom_point() +
theme_minimal() +
labs(x = "GDP per capita", y = "Life Expectancy at Birth") +
scale_x_continuous +  # Apply log scale to x-axis
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.1)))
# try another axis scale
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy)) +
geom_point() +
theme_minimal() +
labs(x = "GDP per capita", y = "Life Expectancy at Birth") +
scale_x_continuous() +  # Apply log scale to x-axis
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.1)))
# try another axis scale
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy)) +
geom_point() +
theme_minimal() +
labs(x = "GDP per capita", y = "Life Expectancy at Birth") +
scale_x_log10() +  # Apply log scale to x-axis
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.1)))
#View(data)
# change opacity globally to fixed value
library(scales)
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy))+
geom_point(alpha = 0.5) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05)))
# use opacity as an encoding channel
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, alpha = Year)) +
geom_point() +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
theme_minimal()
data$Year <- ...
data$Year <- as.factor(data$Year)
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, alpha = Year)) +
geom_point() +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
theme_minimal() +
guides(alpha = guide_legend(title = "Year"))
# Option 1: Using scale_color_viridis_d()
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = Region, shape = Region)) +
geom_point(size = 3) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
scale_color_viridis_d() +
theme_minimal()
knitr::opts_chunk$set(message =  FALSE)
knitr::opts_chunk$set(warning =  FALSE)
knitr::opts_chunk$set(error =  FALSE)
bfcolor <- function(x, color) {
if (knitr::is_latex_output()) {
sprintf("\\textcolor{%s}{\\textbf{%s}}", color, x)
} else if (knitr::is_html_output()) {
sprintf("<span style='color: %s;'><b>%s</b></span>", color, x)
} else x
}
# Install necessary libraries if they aren't installed
if (!require(dplyr)) install.packages("dplyr")
if (!require(tidyr)) install.packages("tidyr")
if (!require(ggplot2)) install.packages("ggplot2")
# Load necessary libraries
library(dplyr)
library(tidyr)
library(readr)
# Import and format country regional information
countryinfo <- read_csv('country-info.csv') %>%
select(3, 6, 7) %>%
rename('Country Code' = 'alpha-3')
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = Year, shape = Year)) +
geom_point(size = 3) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
scale_color_discrete() +
theme_minimal()
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = Year, shape = "diamond")) +
geom_point(size = 3) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
scale_color_viridis_d(alpha = 0.8) +
theme_minimal()
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = Year, shape = "cross")) +
geom_point(size = 3) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
scale_color_brewer(palette = "Set2") +
theme_minimal()
# Option 1: Using scale_color_viridis_d()
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = Region, shape = Region)) +
geom_point(size = 3) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
scale_color_viridis_d(alpha = 0.8) +
theme_minimal()
# Option 1: Using scale_color_viridis_d()
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = Region, shape = "plus")) +
geom_point(size = 3) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
scale_color_viridis_d(alpha = 0.8) +
theme_minimal()
# map population to size
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = region, size = Population)) +
geom_point(alpha = 0.5) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth",expand = expansion(mult = c(0, 0.07))) +
scale_color_brewer(name = "Region",palette = "Set2") +
scale_size_continuous(name = "Population") +
theme_minimal()
# rescale size
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = region, size = Population)) +
geom_point(alpha = 0.5) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth",expand = expansion(mult = c(0, 0.07))) +
scale_color_brewer(name = "Region",palette = "Set2") +
scale_size_continuous(trans = "sqrt",name = "Population") +
theme_minimal()
# facet by year
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = region, size = Population)) +
geom_point(alpha = 0.5) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.07))) +
scale_size_continuous(trans = "sqrt", guide = guide_legend(title = "Population")) +
theme_minimal() +
guides(color = guide_legend(title = "Region")) +
facet_wrap(~ Year, ncol = 1)  # Adjust the number of columns if needed
# Option 1: Using scale_color_viridis_d()
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = region, shape = "plus")) +
geom_point(size = 3) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
scale_color_viridis_d(alpha = 0.8) +
theme_minimal()
# Option 2: Using scale_color_brewer()
ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = region, shape = "square")) +
geom_point(size = 3) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.05))) +
scale_color_brewer(palette = "Set2") +
theme_minimal()
# resize facets
# Create the faceted plot
p <- ggplot(data, aes(x = GDP_per_capita, y = Life_Expectancy, color = region, size = Population)) +
geom_point(alpha = 0.5) +
scale_x_log10() +
scale_y_continuous(name = "Life Expectancy at Birth", expand = expansion(mult = c(0, 0.07))) +
scale_size_continuous(trans = "sqrt", guide = guide_legend(title = "Population")) +
theme_minimal() +
guides(color = guide_legend(title = "Region")) +
facet_wrap(~ Year) +
theme(
panel.spacing = unit(1, "lines"),      # Adjust space between facets
aspect.ratio = 1                    # Adjust aspect ratio of individual facets
)
p
# Save the plot with specific dimensions
#ggsave("faceted_plot.png", plot = p, width = 12, height = 8)
knitr::opts_chunk$set(echo=TRUE,
cache=FALSE,
fig.width=5,
fig.height=5,
fig.align='center')
indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
solcode = TRUE
r = function(x, digits=2){ round(x, digits=digits) }
install.packages("ISLR")
install.packages("tree")
install.packages('maptree')
install.packages("ISLR")
# Load libraries
library(ISLR)
library(tree)
library(maptree)
# Utility library
library(dplyr)
# Create data frame with the oringinal eleven variables and High
Carseats = Carseats %>%
mutate(High=as.factor(ifelse(Sales <= median(Sales), "No", "Yes")))
# Check the structure of above data frame we just created
glimpse(Carseats)
tree.carseats = tree(High ~.-Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0, cex = .4, col = "red")
title("decision tree on Carseats", cex = 0.8)
draw.tree(tree.carseats, nodeinfo=TRUE, cex = 0.4)
# Set random seed for results being reproducible
set.seed(3)
# Get dimension of dataset
dim(Carseats)
# Sample 75% of observations as the training set
train = sample(nrow(Carseats), 0.75*nrow(Carseats))
Carseats.train = Carseats[train, ]
# The rest 25% as the test set
Carseats.test = Carseats[-train,]
# For later convenience in coding, we create High.test, which is the true labels of the
# test cases
High.test = Carseats.test$High
# Fit model on training set
tree.carseats = tree(High~.-Sales, data = Carseats.train)
# Plot the tree
draw.tree(tree.carseats, nodeinfo=TRUE, cex = 0.4)
title("Classification Tree Built on Training Set")
# Predict on test set
tree.pred = predict(tree.carseats, Carseats.test, type="class")
tree.pred
mean(tree.pred != High.test)
# Obtain confusion matrix
error = table(tree.pred, High.test)
error
# Test accuracy rate
sum(diag(error))/sum(error)
# Test error rate (Classification Error)
1-sum(diag(error))/sum(error)
# Set random seed
set.seed(3)
# K-Fold cross validation
cv = cv.tree(tree.carseats, FUN=prune.misclass, K=10)
# Print out cv
cv$size
cv$dev
# Best size
# note that there is a tie
# tree of size 21 and size 16 both have the minimum CV estimate of test error rate
# we prefer the tree with smaller size
best.cv = min(cv$size[cv$dev == min(cv$dev)])
best.cv
# Plot size vs. cross-validation error rate
plot(cv$size , cv$dev, type="b",
xlab = "Number of leaves, \'best\'", ylab = "CV Misclassification Error",
col = "red", main="CV")
abline(v=best.cv, lty=2)
# Add lines to identify complexity parameter
min.error = which.min(cv$dev) # Get minimum error index
abline(h = cv$dev[min.error],lty = 2)
# Prune tree.carseats
pt.cv = prune.misclass(tree.carseats, best=best.cv)
# Plot pruned tree
plot(pt.cv)
text(pt.cv, pretty=0, col = "blue", cex = .5)
title("Pruned tree of size 16")
# Predict on test set
pred.pt.cv = predict(pt.cv, Carseats.test, type="class")
# Obtain confusion matrix
err.pt.cv = table(pred.pt.cv, High.test)
err.pt.cv
# Test accuracy rate
sum(diag(err.pt.cv))/sum(err.pt.cv)
# Test error rate (Classification Error)
1-sum(diag(err.pt.cv))/sum(err.pt.cv)
# Codes start here
set.seed(1162025)
# Perform 5-fold cross-validation
cv_carseats <- cv.tree(tree.carseats, FUN = prune.misclass, K = 5)
# View CV results
cv_carseats
# Plot cross-validation error vs tree size
plot(cv_carseats$size, cv_carseats$dev, type = "b",
xlab = "Tree Size (Number of Terminal Nodes)",
ylab = "Cross-Validation Error",
main = "Cross-Validation Error vs Tree Size")
# Find the best tree size (minimum CV error)
best_size <- cv_carseats$size[which.min(cv_carseats$dev)]
cat("Best tree size:", best_size, "\n")
# Prune the tree to the best size
pruned_tree <- prune.misclass(tree.carseats, best = best_size)
# Plot the pruned tree
plot(pruned_tree)
text(pruned_tree, pretty = 0)
title(paste("Pruned Tree with", best_size, "Terminal Nodes"))
# Codes start here:
# Test set is Carseats.test
# Predict on test set
tree_pred <- predict(pruned_tree, Carseats.test, type = "class")
# Confusion matrix
confusion_matrix <- table(Predicted = tree_pred, Actual = Carseats.test$High)
confusion_matrix
# Calculate test error rate
test_error_rate <- mean(tree_pred != Carseats.test$High)
cat("Test Error Rate:", test_error_rate, "\n")
cat("Test Accuracy:", 1 - test_error_rate, "\n")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dendextend)
library(e1071)
drug <- read_csv('drug.csv',
col_names=c('ID','Age','Gender','Education','Country',
'Ethnicity','Nscore',
'Escore','Oscore','Ascore','Cscore',
'Impulsive','SS','Alcohol','Amphet','Amyl','Benzos',
'Caff','Cannabis', 'Choc','Coke','Crack','Ecstasy',
'Heroin','Ketamine','Legalh','LSD','Meth',
'Mushrooms','Nicotine','Semer','VSA'))
drug <- read_csv('drug.csv',
col_names=c('ID','Age','Gender','Education','Country',
'Ethnicity','Nscore',
'Escore','Oscore','Ascore','Cscore',
'Impulsive','SS','Alcohol','Amphet','Amyl','Benzos',
'Caff','Cannabis', 'Choc','Coke','Crack','Ecstasy',
'Heroin','Ketamine','Legalh','LSD','Meth',
'Mushrooms','Nicotine','Semer','VSA'))
setwd("C:/Users/ktchi/Downloads/pstat131")
drug <- read_csv('drug.csv',
col_names=c('ID','Age','Gender','Education','Country',
'Ethnicity','Nscore',
'Escore','Oscore','Ascore','Cscore',
'Impulsive','SS','Alcohol','Amphet','Amyl','Benzos',
'Caff','Cannabis', 'Choc','Coke','Crack','Ecstasy',
'Heroin','Ketamine','Legalh','LSD','Meth',
'Mushrooms','Nicotine','Semer','VSA'))
getwd()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dendextend)
library(e1071)
leukemia_data <- read_csv("leukemia_data.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dendextend)
library(e1071)
leukemia_data <- read_csv("leukemia_data.csv")
drug <- read_csv('drug.csv',
col_names=c('ID','Age','Gender','Education','Country',
'Ethnicity','Nscore',
'Escore','Oscore','Ascore','Cscore',
'Impulsive','SS','Alcohol','Amphet','Amyl','Benzos',
'Caff','Cannabis', 'Choc','Coke','Crack','Ecstasy',
'Heroin','Ketamine','Legalh','LSD','Meth',
'Mushrooms','Nicotine','Semer','VSA'))
View(drug)
set.seed(12052025)
train_indices <- sample(nrow(drug), 1000)
train_data <- drug[train_indices, ]
test_data <- drug[-train_indices, ]
train_data$recent_nicotine_use <- as.factor(ifelse(train_data$Nicotine >= "CL3", "Yes", "No"))
test_data$recent_nicotine_use <- as.factor(ifelse(test_data$Nicotine >= "CL3", "Yes", "No"))
#predictors <- c('Age', 'Gender', 'Education', 'Country', #'Ethnicity',
#                'Nscore', 'Escore', 'Oscore', 'Ascore', #'Cscore',
#                'Impulsive', 'SS')
# Fit SVM with radial kernel and cost = 1
svmfit <- svm(recent_nicotine_use ~ Age + Gender + Education + Country +
Ethnicity + Nscore + Escore + Oscore + Ascore + Cscore +
Impulsive + SS,
data = train_data,
kernel = "radial",
cost = 1)
# Generate predictions on test data
ypred <- predict(svmfit, test_data)
# Print confusion matrix
confusion_matrix_a <- table(predict = ypred, truth = test_data$recent_nicotine_use)
print(confusion_matrix_a)
# Calculate test error rate
test_error_a <- 1 - sum(diag(confusion_matrix_a)) / sum(confusion_matrix_a)
set.seed(12052025)
train_indices <- sample(nrow(drug), 1000)
train_data <- drug[train_indices, ]
test_data <- drug[-train_indices, ]
train_data$recent_nicotine_use <- as.factor(ifelse(train_data$Nicotine >= "CL3", "Yes", "No"))
test_data$recent_nicotine_use <- as.factor(ifelse(test_data$Nicotine >= "CL3", "Yes", "No"))
# Fit SVM with radial kernel and cost = 1
svmfit <- svm(recent_nicotine_use ~ Age + Gender + Education + Country +
Ethnicity + Nscore + Escore + Oscore + Ascore + Cscore +
Impulsive + SS,
data = train_data,
kernel = "radial",
cost = 1)
# Generate predictions on test data
ypred <- predict(svmfit, test_data)
# Print confusion matrix
confusion_matrix_a <- table(predict = ypred, truth = test_data$recent_nicotine_use)
print(confusion_matrix_a)
# Calculate test error rate
test_error_a <- 1 - sum(diag(confusion_matrix_a)) / sum(confusion_matrix_a)
test_error_a
tune.out <- tune(svm,
recent_nicotine_use ~ Age + Gender + Education + Country +
Ethnicity + Nscore + Escore + Oscore + Ascore + Cscore +
Impulsive + SS,
data = train_data,
kernel = "radial",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 100)))
# Print summary of cross-validation results
summary(tune.out)
# Extract optimal cost and corresponding CV error
optimal_cost <- tune.out$best.parameters$cost
cv_error <- tune.out$best.performance
optimal_cost
cv_error
# Get the best model
bestmod <- tune.out$best.model
# Generate predictions using the best model
ypred_best <- predict(bestmod, test_data)
# Print confusion matrix for best model
confusion_matrix_b <- table(predict = ypred_best, truth = test_data$recent_nicotine_use)
confusion_matrix_b
tune.out <- tune(svm,
recent_nicotine_use ~ Age + Gender + Education + Country +
Ethnicity + Nscore + Escore + Oscore + Ascore + Cscore +
Impulsive + SS,
data = train_data,
kernel = "radial",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10, 100)))
# Print summary of cross-validation results
summary(tune.out)
# Extract optimal cost and corresponding CV error
optimal_cost <- tune.out$"best.parameters"$cost
cv_error <- tune.out$"best.performance"
optimal_cost
cv_error
# Get the best model
bestmod <- tune.out$best.model
# Generate predictions using the best model
ypred_best <- predict(bestmod, test_data)
# Print confusion matrix for best model
confusion_matrix_b <- table(predict = ypred_best, truth = test_data$recent_nicotine_use)
confusion_matrix_b
knitr::opts_chunk$set(echo = TRUE)
leukemia_data$Type <- as.factor(leukemia_data$Type)
getwd()
leukemia_data$Type <- as.factor(leukemia_data$Type)
leukemia_data$Type <- as.factor(leukemia_data$Type)
leukemia_data <- read_csv("leukemia_data.csv")
leukemia_data <- read_csv("leukemia_data.csv")
setwd("C:/Users/ktchi/OneDrive/Desktop/pstat131")
leukemia_data <- read_csv("leukemia_data.csv")
# install.packages("cluster")
library(cluster)
library(tidyverse)
# Read tab-delimited file by read.delim
car = read_delim("./cars.tab", delim = "\t")
# First 3 observations
head(car, 3)
# Standardize the variables by subtracting mean and divided by standard deviation
scar = scale(car[, -c(1,2)], center=TRUE, scale=TRUE)
head(scar)
# install.packages("dendextend")
library(dendextend)
## dendrogram: branches colored by 3 groups
dend1 = as.dendrogram(car.hclust)
# install.packages("cluster")
library(cluster)
library(tidyverse)
# Read tab-delimited file by read.delim
car = read_delim("./cars.tab", delim = "\t")
# First 3 observations
head(car, 3)
# Standardize the variables by subtracting mean and divided by standard deviation
scar = scale(car[, -c(1,2)], center=TRUE, scale=TRUE)
head(scar)
set.seed(1)
# 3-means clustering
km = kmeans(scar, centers=3)
km
# install.packages("factoextra")
library(factoextra)
fviz_cluster(km, data = scar, geom = "point",
stand = FALSE, frame.type = "norm")
# First, we use daisy to calculate the distance matrix
car.dist = daisy(scar)
# can also do: car.dist = dist(scar)
# 3-medoids clustering -->
scar.pam = pam(scar, k=3, keep.diss=TRUE)
scar.pam
# We use the dist function in this example.
car.dist = dist(scar)
# Can also do: car.dist = daisy(scar)
# Agglomerative Hierarchical clutering
set.seed(1)
car.hclust = hclust(car.dist, method = ) # complete linkage
# check methods in hclust documentation: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust
# Can also do: car.hclust = agnes(car.dist)
# Plot dendogram
plot(car.hclust, labels=car$Car, main='Default from hclust')
# Add a horizontal line at a certain height
abline(h=6, col="red", lty=2)
abline(h=3.5, col="green", lty=2)
clus = cutree(car.hclust, 3)
clus
clus = cutree(car.hclust, 3)
clus
table(clus)
# install.packages("dendextend")
library(dendextend)
## dendrogram: branches colored by 3 groups
dend1 = as.dendrogram(car.hclust)
# color branches and labels by 3 clusters
dend1 = color_branches(dend1, k=3)
dend1 = color_labels(dend1, k=3)
# change label size
dend1 = set(dend1, "labels_cex", 0.3)
# add true labels to observations
dend1 = set_labels(dend1, labels=car$Car[order.dendrogram(dend1)])
# plot the dendrogram
plot(dend1, horiz=T, main = "Dendrogram colored by three clusters")
View(car)
reticulate::repl_python()
knitr::opts_chunk$set(echo = TRUE)
install.packages("reticulate")
install.packages("reticulate")
reticulate::repl_python()
knitr::opts_chunk$set(echo = TRUE)
reticulate::repl_python()
